# ML-Interview
Банк вопросов с собеседований на позицию Machine Learning Engineer в области Computer Vision.

1. Почему модель на этапе обучения занимает больше памяти, чем на инференсе?
2. Что такое Dropout?
3. Какие виды нормализации существуют?
4. Как работает BatchNorm и LayerNorm? Преимущества и недостатки каждой и где применяются?
5. Рассказать про архитектуру ViT.
6. Что такое переобучение и способы борьбы с ним.
7. Как работает NMS (Non Maximum Suppression) и для чего нужен?
8. Рассказать про метрику MAP (Mean Average Precision). Что значит MAP@0.5, MAP@0.5:0.95?
9. Какие существуют методы регуляризации? Плюсы и минус каждого, области применения каждого.
10. Почему L1 регуляризация зануляет часть весов?
11. Какие знаешь оптимизаторы, в чем их идеи и различия?
12. Gradient Descent (GD), Stochastic Gradient Descent (SGD) и Mini-Batch Stochastic Gradient Descent (Mini-Batch SGD). В чем их различия, плюсы и минусы каждого?
13. Если бы мы имели бесконечные ресурсы (память, GPU, CPU и т.д.), а также нам была бы не важна скорость сходимсти, то какой метод GD, SGD или Mini-Batch SGD лучше использовать?
14. Какие проблемы могут возникать при использовании функции активации Sigmoid вместе с BatchNorm?
15. Проблемы функции активации Sigmoid, где применяется и как интерпретируется?
16. Рассказать про multi-head attention в деталях.
17. CLIP - идея, функция потерь, способ обучения, для чего используется? Какие метрики дистанции между эмбеддингами можно использовать и какую метрику использовали авторы?
18. Что является таргетом в задаче детекции, как формируется функция потерь?
19. Какие типы моделей знаешь для задач детекции?
20. В чем разница self-attention и cross-attention, для чего используется каждый?
21. Какие знаешь трансформеры для задач Computer Vision (CV)?
22. Как из текстов получить эмбеддинги, которые пойдут на вход в трансформер?
23. Проблемы с инициализацией весов нулями. Как можно инициализировать веса так, чтобы решить проблемы?
24. Формулировка задачи Maximum Likelihood Estimation. Записать формулу.
25. Рассказать про идею ResNet, написать ResidualBlock.
26. Что такое операция свертки? Какие у нее свойства? Как представить в виде матричного умножения?
27. Что такое receptive field?
28. Как можно аггрегировать композитную функцию потерь (loss = loss1 + loss2)?
29. Что означает каждое обозначение в функции потерь для Faster-RCNN (loss_classifier, loss_objectness, loss_rpn, loss_bbox)?
30. Чем двухстадийные детекторы отличаются от одностадийных?
31. Как аналитически решается задача линейной регрессии?
32. Pre-layer norm vs post-layer norm, в чем разница, какие и где используются и почему?
33. Задачка на матстат: есть 100 монет, 1 нечестная (обе стороны орел), выпал орел, найдите вероятность, что монетка была нечестной.
34. Почему сеть с BatchNorm сходится быстрее?
35. Какие виды сверток знаешь, идеи, плюсы и минусы каждой?
36. Какие обучаемые параметры есть в BatchNorm и для чего они нужны?
37. Какие метрики бинарной классификации есть? Плюсы и минусы каждой.
38. Что такое TPR и FPR?
39. Как ROC-AUC работает на данных, где есть дисбаланс классов?
40. Что такое проблема мультиколлинеарности признаков?
41. Что такое bias, variance модели? Что такое bias-variance trade-off? Какой bias и variance у различных типов моделей: линейные модели, деревья, ансамбли деревьев?
42. Рассказать про RandomForest.
43. Что такое градиентный бустинг? Где там появляется градиент?
44. Почему в градиентном бустинге обычно менее глубокие деревья используют, чем в случайном лесе?
45. Почему деревья сильнее переобучаются?
46. Можно ли строить случайный лес над KNN, линейными моделями и нейросетями, почему?
47. Задача: решаем задачу линейной регрессии, все y > 0, какие алгоритмы из написанных могут дать отрицательное значение: линейная регрессия, KNN, градиентный бустинг, дерево, случайный лес, нейросеть?
48. ROC-AUC = 0.9, что с ним будет если домножить все предсказания на число 3?
49. Есть градиентный бустинг и случайный лес на 1000 деревьев, что будет с качеством каждой модели если удалить первое построенное дерево?
50. Коэффициент корреляции равен 0 можно ли утверждать, что выборки независимы?
51. Какую зависимость ищет корреляция?
52. Как проверить нормальность выборки?
53. Метрики multiclass классификации, их плюсы и минусы?
54. Как происходит процесс построения дерева?
55. Что такое дисбаланс классов и как с ним бороться?
56. Почему мы перешли от сверток (CNN) к механизму вниманию (Transformer) во многих задачах CV?
57. Почему практически перестали использовать свертки больших размеров 9x9, 7x7, 5x5?
58. Может ли быть такое, что Atrous свертка вообще никогда не использует какой-то пиксель?
59. Можно ли заменить свертку 3x3 на две: 3x1 и 1x3?
60. Как работает матчинг bounding bbox в моделях YOLO / DETR?
61. Рассказать про венгерский алгоритм.
62. Как обучался BERT?
63. Как работают различные токенизаторы текста?
64. Почему в трансформерах есть ограничения на количество токенов?
65. Что такое позиционные эмбеддинги и для чего они нужны? Какие есть виды и для чего каждый нужен?
66. 
