# ML-Interview
Банк вопросов с собеседований на позицию Machine Learning Engineer в области Computer Vision.

1. Почему модель на этапе обучения занимает больше памяти, чем на инференсе?
2. Что такое Dropout?
3. Какие виды нормализации существуют?
4. Как работает BatchNorm и LayerNorm? Преимущества и недостатки каждой и где применяются?
5. Рассказать про архитектуру ViT.
6. Что такое переобучение и способы борьбы с ним.
7. Как работает NMS (Non Maximum Suppression) и для чего нужен?
8. Рассказать про метрику MAP (Mean Average Precision). Что значит MAP@0.5, MAP@0.5:0.95?
9. Какие существуют методы регуляризации? Плюсы и минус каждого, области применения каждого.
10. Почему L1 регуляризация зануляет часть весов?
11. Какие знаешь оптимизаторы, в чем их идеи и различия?
12. Gradient Descent (GD), Stochastic Gradient Descent (SGD) и Mini-Batch Stochastic Gradient Descent (Mini-Batch SGD). В чем их различия, плюсы и минусы каждого?
13. Если бы мы имели бесконечные ресурсы (память, GPU, CPU и т.д.), а также нам была бы не важна скорость сходимсти, то какой метод GD, SGD или Mini-Batch SGD лучше использовать?
14. Какие проблемы могут возникать при использовании функции активации Sigmoid вместе с BatchNorm?
15. Проблемы функции активации Sigmoid, где применяется и как интерпретируется?
16. Рассказать про multi-head attention в деталях.
17. CLIP - идея, функция потерь, способ обучения, для чего используется? Какие метрики дистанции между эмбеддингами можно использовать и какую метрику использовали авторы?
18. Что является таргетом в задаче детекции, как формируется функция потерь?
19. Какие типы моделей знаешь для задач детекции?
20. В чем разница self-attention и cross-attention, для чего используется каждый?
21. Какие знаешь трансформеры для задач Computer Vision (CV)?
22. Как из текстов получить эмбеддинги, которые пойдут на вход в трансформер?
23. Проблемы с инициализацией весов нулями. Как можно инициализировать веса так, чтобы решить проблемы?
24. Формулировка задачи Maximum Likelihood Estimation. Записать формулу.
25. Рассказать про идею ResNet, написать ResidualBlock.
26. Что такое операция свертки? Какие у нее свойства? Как представить в виде матричного умножения?
27. Что такое receptive field?
28. Как можно аггрегировать композитную функцию потерь (loss = loss1 + loss2)?
29. Что означает каждое обозначение в функции потерь для Faster-RCNN (loss_classifier, loss_objectness, loss_rpn, loss_bbox)?
30. Чем двухстадийные детекторы отличаются от одностадийных?
31. Как аналитически решается задача линейной регрессии?
32. Pre-layer norm vs post-layer norm, в чем разница, какие и где используются и почему?
33. Задачка на матстат: есть 100 монет, 1 нечестная (обе стороны орел), выпал орел, найдите вероятность, что монетка была нечестной.
34. Почему сеть с BatchNorm сходится быстрее?
35. Какие виды сверток знаешь, идеи, плюсы и минусы каждой?
36. Какие обучаемые параметры есть в BatchNorm и для чего они нужны?
37. Какие метрики бинарной классификации есть? Плюсы и минусы каждой.
38. Что такое TPR и FPR?
39. Как ROC-AUC работает на данных, где есть дисбаланс классов?
40. Что такое проблема мультиколлинеарности признаков?
41. Что такое bias, variance модели? Что такое bias-variance trade-off? Какой bias и variance у различных типов моделей: линейные модели, деревья, ансамбли деревьев?
42. Рассказать про RandomForest.
43. Что такое градиентный бустинг? Где там появляется градиент?
44. Почему в градиентном бустинге обычно менее глубокие деревья используют, чем в случайном лесе?
45. Почему деревья сильнее переобучаются?
46. Можно ли строить случайный лес над KNN, линейными моделями и нейросетями, почему?
47. Задача: решаем задачу линейной регрессии, все y > 0, какие алгоритмы из написанных могут дать отрицательное значение: линейная регрессия, KNN, градиентный бустинг, дерево, случайный лес, нейросеть?
48. ROC-AUC = 0.9, что с ним будет если домножить все предсказания на число 3?
49. Есть градиентный бустинг и случайный лес на 1000 деревьев, что будет с качеством каждой модели если удалить первое построенное дерево?
50. Коэффициент корреляции равен 0 можно ли утверждать, что выборки независимы?
51. Какую зависимость ищет корреляция?
52. Как проверить нормальность выборки?
53. Метрики multiclass классификации, их плюсы и минусы?
54. Как происходит процесс построения дерева?
55. Что такое дисбаланс классов и как с ним бороться?
56. Почему мы перешли от сверток (CNN) к механизму вниманию (Transformer) во многих задачах CV?
57. Почему практически перестали использовать свертки больших размеров 9x9, 7x7, 5x5?
58. Может ли быть такое, что Atrous свертка вообще никогда не использует какой-то пиксель?
59. Можно ли заменить свертку 3x3 на две: 3x1 и 1x3?
60. Как работает матчинг bounding bbox в моделях YOLO / DETR?
61. Рассказать про венгерский алгоритм.
62. Как обучался BERT?
63. Как работают различные токенизаторы текста?
64. Почему в трансформерах есть ограничения на количество токенов?
65. Что такое позиционные эмбеддинги и для чего они нужны? Какие есть виды и для чего каждый нужен?
66. Что такое gradient clipping?
67. Рассказать про gradient accumulation.
68. Рассказать про MobileNet и EfficientNet.
69. Что такое bottleneck слой и для чего он используется?
70. Почему в residual connection используется операция сложения?
71. Можно ли использовать двумерное позиционное кодирование в трансформерах, работающих с изображениями?
72. Какие проблемы могут быть у одномерного позиционного кодирования в случае работы с изображениями?
73. Задача: что будет эффективнее подать 2 входа по 512 токенов в трансформер по отдельности (2x512) или объединить входы и подать сразу (1x1024)?
74. Как борются с квадратичной сложностью механизма внимания?
75. Что такое и зачем нужны RoI, RoI Pooling, RoI Align?
76. Для чего в задачах детекции нужны anchor боксы?
77. Как объединяются выходы голов в FPN?
78. Что такое multi-scale детекция?
79. Какая вероятностная интерпретация у ROC-AUC?
80. Как происходит расчет ROC-AUC?
81. Как определяются thresholds для расчета ROC-AUC?
82. Решаем задачу бинарной классификации, как изменятся метрики precision и recall, если выкинуть 10 нулей из таргета?
83. Что такое bagging?
84. Что такое градиент?
85. Что такое pvalue и для чего оно нужно?
86. Отличие classmethod от staticmethod в Python?
87. Какие типы подходят как ключ словаря в Python?
88. Можно ли использовать (1, [1,2]) как ключ словаря в Python?
89. Какая структура данных лежит в основе dict в Python?
90. Какие знаешь виды сортировок и их сложности?
91. Какая сложность вставки в словарь и индексации в Python?
92. Как посчитать медиану в SQL без встроенной функции? Как посчитать количество строк в таблице? Знаешь ли ты про оконные функции?
93. Какой learning rate будешь использовать для большого батча, а какой для маленького?
94. Что такое GIL в Python?
95. Как реализуют параллельность с учетом GIL в Python?
96. Что такое ассинхронность?
97. Какие слои в нейросетях отличаются поведением на трейне / инференсе?
98. Что такое GAN, какие знаешь, какие использовал?
99. Какая математическая идея у диффузионных моделей?
100. Какая идея Cycle-GAN?
101. Какие методы можно использовать для генерации изображений? Их плюсы и минусы.
102. Рассказать про ControlNet, LoRA.
103. Из каких частей состоит Stable Diffusion (SD)?
104. Что предсказывает U-Net в SD на каждом шаге?
105. Какой вид U-Net используется в SD?
106. Как обучаются модели SD?
107. Как семплируется шаг t в SD?
108. Как можно модифицировать архитектуру SD, чтобы генерировать не только по тексту, но и по дополнительным входам (изображения, аудио и т.д.)?
109. Какие еще диффузионные модели есть кроме SD?
